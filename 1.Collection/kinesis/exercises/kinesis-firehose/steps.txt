1- Create S3 Bucket
    -Select a name for Order Logs

2 -Create Kinesis Firehose (delivery stream)
    -Name: PurchaseLogs (Or anyone)
    -Source -> Direct PUT or other sources
    -Record transformation -> Disabled
    -Record format conversion -> Disabled
    -Destination -> Amazon S3
    -Select S3 bucket created previously
    -Buffer Size -> let the default of 5 Mb
    -Buffer interval -> set 60 seconds (the minimum value)
    -S3 compression -> Disabled
    -S3 encryption -> Disabled
    -Error logging -> Enabled
    -IAM role -> Create New

3 - Create new key pair for ec2 instance

4 - Create Ec2 Instance
    -Amazon Linux AMI
    -t2-micro
    -Select created key pair

5 - Login into Ec2 Instance
    - Run "sudo yum install -y aws-kinesis-agent"
    - Run "wget http://media.sundog-soft.com/AWSBigData/LogGenerator.zip"
    - Run "unzip LogGenerator.zip"
    - Run "chmod a+x LogGenerator.py"
    - Run "sudo mkdir /var/log/cadabra"
    - Run "cd /etc/aws-kinesis/"
    - Run "sudo nano agent.json"
        -Set firehose.endpoint in blank if kinesis is in the same region, 
            else put "firehose.us-east-1.amazonaws.com" (with the region)
        -put below:
            "awsAccessKeyId" : MY_ACCESS_KEY,
            "awsSecretAccessKey": MY_ACCESS_SECRET

            ////Also can attach the ec2 service to a IAM role
        - Delete the first flow (It's for kinesis data streams)
        -Change "filePattern": "/var/log/cadabra/*log"
        -Change "deliveryStream": "PurchaseLogs"
        -Exit Saving all
    - Run "sudo service aws-kinesis-agent start"
    - Run "sudo chkconfig aws-kinesis-agent on"
    - Run "cd ~"
    - Run "sudo ./LogGenerator.py 500000"
    - Run "cd /var/log/cadabra/" -> "ls"
    - Run "tail -f /var/log/aws-kinesis-agent/aws-kinesis-agent.log"
    - Now can check the bucket and check if there're the logs

